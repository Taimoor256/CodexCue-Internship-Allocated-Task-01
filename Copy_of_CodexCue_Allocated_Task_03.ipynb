{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPUk01Skm6Syk0fD7mPdhSx",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Taimoor256/CodexCue-Internship-Allocated-Task-01/blob/main/Copy_of_CodexCue_Allocated_Task_03.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **MOUNT DRIVE**"
      ],
      "metadata": {
        "id": "dYUvuPQ61CAl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R-PtbS5bmzFA",
        "outputId": "38c78777-75f2-4134-b5ff-69aa94c6863c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Import Libraries**"
      ],
      "metadata": {
        "id": "tVq1RApaHOzJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV, RandomizedSearchCV, cross_val_score\n",
        "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
        "from scipy.stats import uniform, randint"
      ],
      "metadata": {
        "id": "UJ8_nHlkHYt_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Load Dataset and Hyper parameters tunning on Three algorithms that used in Task 2**"
      ],
      "metadata": {
        "id": "aE7JTrZTHaJu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1: Data Preparation\n",
        "# Load the dataset\n",
        "df = pd.read_csv('/content/drive/MyDrive/emails.csv')\n",
        "\n",
        "# Handling missing values\n",
        "df.fillna(method='ffill', inplace=True)\n",
        "\n",
        "# Encoding categorical variables\n",
        "label_encoders = {}\n",
        "for column in df.select_dtypes(include=['object']).columns:\n",
        "    label_encoders[column] = LabelEncoder()\n",
        "    df[column] = label_encoders[column].fit_transform(df[column])\n",
        "\n",
        "# Scaling numerical features\n",
        "scaler = StandardScaler()\n",
        "numerical_columns = df.select_dtypes(include=['int64', 'float64']).columns\n",
        "df[numerical_columns] = scaler.fit_transform(df[numerical_columns])\n",
        "\n",
        "# Separating features and target variable\n",
        "print(df.columns)  # Print columns to identify the correct target variable column\n",
        "X = df.drop('spam', axis=1)  # Replace 'spam' with the actual column name\n",
        "\n",
        "# Check if 'spam' column is numerical and convert to categories if needed\n",
        "if df['spam'].dtype in ['int64', 'float64']:\n",
        "    # Assuming spam is binary (0 or 1), adjust threshold as needed\n",
        "    df['spam'] = df['spam'].apply(lambda x: 1 if x > 0.5 else 0)\n",
        "y = df['spam']  # Replace 'spam' with the actual column name\n",
        "\n",
        "# Step 2: Model Selection\n",
        "models = {\n",
        "    'Logistic Regression': LogisticRegression(),\n",
        "    'Decision Tree': DecisionTreeClassifier(),\n",
        "    'Support Vector Machine': SVC(),\n",
        "    'Random Forest': RandomForestClassifier(random_state=42),\n",
        "    'Gradient Boosting': GradientBoostingClassifier(random_state=42)\n",
        "}\n",
        "\n",
        "# Step 3: Hyperparameter Tuning\n",
        "# Define hyperparameter grids for each model\n",
        "param_grids = {\n",
        "    'Logistic Regression': {\n",
        "        'C': [0.01, 0.1, 1, 10, 100],\n",
        "        'penalty': ['l2'],\n",
        "        'solver': ['lbfgs', 'liblinear']\n",
        "    },\n",
        "    'Decision Tree': {\n",
        "        'max_depth': [None, 10, 20, 30],\n",
        "        'min_samples_split': [2, 5, 10],\n",
        "        'min_samples_leaf': [1, 2, 4]\n",
        "    },\n",
        "    'Support Vector Machine': {\n",
        "        'C': [0.1, 1, 10, 100],\n",
        "        'kernel': ['linear', 'rbf', 'poly'],\n",
        "        'gamma': ['scale', 'auto']\n",
        "    },\n",
        "    'Random Forest': {\n",
        "        'n_estimators': [50, 100, 150],\n",
        "        'max_depth': [None, 10, 20, 30],\n",
        "        'min_samples_split': [2, 5, 10],\n",
        "        'min_samples_leaf': [1, 2, 4]\n",
        "    },\n",
        "    'Gradient Boosting': {\n",
        "        'n_estimators': [50, 100, 150],\n",
        "        'max_depth': [3, 5, 7],\n",
        "        'learning_rate': [0.01, 0.1, 0.2]\n",
        "    }\n",
        "}\n",
        "\n",
        "# Step 4: Model Training with Hyperparameter Tuning\n",
        "# Splitting the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "best_models = {}\n",
        "\n",
        "# Performing Grid Search for each model\n",
        "for name, model in models.items():\n",
        "    print(f\"Tuning hyperparameters for {name}...\")\n",
        "\n",
        "    grid_search = GridSearchCV(model, param_grids[name], cv=5, scoring='accuracy', verbose=2, n_jobs=-1)\n",
        "    grid_search.fit(X_train, y_train)\n",
        "\n",
        "    best_models[name] = grid_search.best_estimator_\n",
        "\n",
        "    print(f\"Best Parameters for {name}: {grid_search.best_params_}\")\n",
        "    print(f\"Best Cross-Validation Accuracy for {name}: {grid_search.best_score_}\")\n",
        "    print(\"\\n\" + \"=\"*60 + \"\\n\")\n",
        "\n",
        "# Step 5: Model Evaluation\n",
        "# Evaluating the best model for each algorithm\n",
        "for name, model in best_models.items():\n",
        "    y_pred = model.predict(X_test)\n",
        "\n",
        "    accuracy = accuracy_score(y_test, y_pred)\n",
        "    print(f\"Model: {name}\")\n",
        "    print(f\"Accuracy: {accuracy}\")\n",
        "\n",
        "    print(\"Classification Report:\")\n",
        "    print(classification_report(y_test, y_pred))\n",
        "    print(\"\\n\" + \"=\"*60 + \"\\n\")\n",
        "\n",
        "# Pro Tip: Use RandomizedSearchCV for larger hyperparameter spaces\n",
        "# Example: Random Search for SVM\n",
        "\n",
        "random_search_params = {\n",
        "    'C': uniform(0.1, 100),\n",
        "    'kernel': ['linear', 'rbf', 'poly'],\n",
        "    'gamma': ['scale', 'auto']\n",
        "}\n",
        "\n",
        "random_search = RandomizedSearchCV(SVC(), random_search_params, n_iter=10, cv=5, scoring='accuracy', random_state=42, verbose=2, n_jobs=-1)\n",
        "random_search.fit(X_train, y_train)\n",
        "\n",
        "print(\"Best Parameters from Random Search for SVM:\", random_search.best_params_)\n",
        "print(\"Best Cross-Validation Score from Random Search for SVM:\", random_search.best_score_)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J1NAT1OFp-tW",
        "outputId": "93b20f04-0bf2-40ad-a4dd-9d8159712f0c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-25-d621a3be77c9>:17: FutureWarning: DataFrame.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
            "  df.fillna(method='ffill', inplace=True)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Index(['Email No.', 'the', 'to', 'ect', 'and', 'for', 'of', 'a', 'you', 'hou',\n",
            "       ...\n",
            "       'connevey', 'jay', 'valued', 'lay', 'infrastructure', 'military',\n",
            "       'allowing', 'ff', 'dry', 'Prediction'],\n",
            "      dtype='object', length=3002)\n",
            "Tuning hyperparameters for Logistic Regression...\n",
            "Fitting 5 folds for each of 10 candidates, totalling 50 fits\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/joblib/externals/loky/process_executor.py:752: UserWarning: A worker stopped while some jobs were given to the executor. This can be caused by a too short worker timeout or by a memory leak.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best Parameters for Logistic Regression: {'C': 0.01, 'penalty': 'l2', 'solver': 'lbfgs'}\n",
            "Best Cross-Validation Accuracy for Logistic Regression: 0.9954077072709111\n",
            "\n",
            "============================================================\n",
            "\n",
            "Tuning hyperparameters for Decision Tree...\n",
            "Fitting 5 folds for each of 36 candidates, totalling 180 fits\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/joblib/externals/loky/process_executor.py:752: UserWarning: A worker stopped while some jobs were given to the executor. This can be caused by a too short worker timeout or by a memory leak.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best Parameters for Decision Tree: {'max_depth': 10, 'min_samples_leaf': 2, 'min_samples_split': 5}\n",
            "Best Cross-Validation Accuracy for Decision Tree: 0.9939578477589096\n",
            "\n",
            "============================================================\n",
            "\n",
            "Tuning hyperparameters for Support Vector Machine...\n",
            "Fitting 5 folds for each of 24 candidates, totalling 120 fits\n",
            "Best Parameters for Support Vector Machine: {'C': 10, 'gamma': 'auto', 'kernel': 'poly'}\n",
            "Best Cross-Validation Accuracy for Support Vector Machine: 0.9941993936526294\n",
            "\n",
            "============================================================\n",
            "\n",
            "Tuning hyperparameters for Random Forest...\n",
            "Fitting 5 folds for each of 108 candidates, totalling 540 fits\n",
            "Best Parameters for Random Forest: {'max_depth': None, 'min_samples_leaf': 1, 'min_samples_split': 2, 'n_estimators': 50}\n",
            "Best Cross-Validation Accuracy for Random Forest: 0.9939572636092272\n",
            "\n",
            "============================================================\n",
            "\n",
            "Tuning hyperparameters for Gradient Boosting...\n",
            "Fitting 5 folds for each of 27 candidates, totalling 135 fits\n",
            "Best Parameters for Gradient Boosting: {'learning_rate': 0.1, 'max_depth': 7, 'n_estimators': 100}\n",
            "Best Cross-Validation Accuracy for Gradient Boosting: 0.9944412316211906\n",
            "\n",
            "============================================================\n",
            "\n",
            "Model: Logistic Regression\n",
            "Accuracy: 0.996135265700483\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      1.00      1.00      1024\n",
            "           1       0.89      0.73      0.80        11\n",
            "\n",
            "    accuracy                           1.00      1035\n",
            "   macro avg       0.94      0.86      0.90      1035\n",
            "weighted avg       1.00      1.00      1.00      1035\n",
            "\n",
            "\n",
            "============================================================\n",
            "\n",
            "Model: Decision Tree\n",
            "Accuracy: 0.9971014492753624\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      1.00      1.00      1024\n",
            "           1       1.00      0.73      0.84        11\n",
            "\n",
            "    accuracy                           1.00      1035\n",
            "   macro avg       1.00      0.86      0.92      1035\n",
            "weighted avg       1.00      1.00      1.00      1035\n",
            "\n",
            "\n",
            "============================================================\n",
            "\n",
            "Model: Support Vector Machine\n",
            "Accuracy: 0.996135265700483\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      1.00      1.00      1024\n",
            "           1       0.89      0.73      0.80        11\n",
            "\n",
            "    accuracy                           1.00      1035\n",
            "   macro avg       0.94      0.86      0.90      1035\n",
            "weighted avg       1.00      1.00      1.00      1035\n",
            "\n",
            "\n",
            "============================================================\n",
            "\n",
            "Model: Random Forest\n",
            "Accuracy: 0.9971014492753624\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      1.00      1.00      1024\n",
            "           1       1.00      0.73      0.84        11\n",
            "\n",
            "    accuracy                           1.00      1035\n",
            "   macro avg       1.00      0.86      0.92      1035\n",
            "weighted avg       1.00      1.00      1.00      1035\n",
            "\n",
            "\n",
            "============================================================\n",
            "\n",
            "Model: Gradient Boosting\n",
            "Accuracy: 0.9951690821256038\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      1.00      1.00      1024\n",
            "           1       0.80      0.73      0.76        11\n",
            "\n",
            "    accuracy                           1.00      1035\n",
            "   macro avg       0.90      0.86      0.88      1035\n",
            "weighted avg       0.99      1.00      1.00      1035\n",
            "\n",
            "\n",
            "============================================================\n",
            "\n",
            "Fitting 5 folds for each of 10 candidates, totalling 50 fits\n",
            "Best Parameters from Random Search for SVM: {'C': 15.701864044243651, 'gamma': 'scale', 'kernel': 'poly'}\n",
            "Best Cross-Validation Score from Random Search for SVM: 0.9939578477589096\n"
          ]
        }
      ]
    }
  ]
}